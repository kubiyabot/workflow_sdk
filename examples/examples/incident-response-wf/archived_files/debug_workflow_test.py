#!/usr/bin/env python3
"""
Enhanced debugging test for incident response workflow.
Provides real-time streaming, detailed logging, and step-by-step verification.
"""

import os
import sys
import json
import time
from pathlib import Path
from datetime import datetime

# Add the workflow_sdk to the path
sys.path.insert(0, str(Path(__file__).parent.parent))

from kubiya_workflow_sdk.client import KubiyaClient
from fixed_incident_workflow import create_fixed_incident_workflow


class WorkflowDebugger:
    """Enhanced debugging and monitoring for workflow execution."""
    
    def __init__(self, api_key: str):
        self.api_key = api_key
        self.client = KubiyaClient(api_key=api_key)
        self.execution_log = []
        self.step_statuses = {}
        self.start_time = None
        self.events_received = 0
    
    def log(self, level: str, message: str, data: dict = None):
        """Enhanced logging with timestamps and structure."""
        timestamp = datetime.now().strftime("%H:%M:%S.%f")[:-3]
        log_entry = {
            "timestamp": timestamp,
            "level": level,
            "message": message,
            "data": data or {}
        }
        self.execution_log.append(log_entry)
        
        # Color coding for console output
        colors = {
            "INFO": "\033[36m",    # Cyan
            "SUCCESS": "\033[32m", # Green
            "WARNING": "\033[33m", # Yellow
            "ERROR": "\033[31m",   # Red
            "STEP": "\033[35m",    # Magenta
            "EVENT": "\033[94m"    # Blue
        }
        
        color = colors.get(level, "\033[0m")
        reset = "\033[0m"
        
        print(f"{color}[{timestamp}] {level}: {message}{reset}")
        if data:
            print(f"  ğŸ“Š Data: {json.dumps(data, indent=2, default=str)[:200]}...")
    
    def test_workflow_creation(self):
        """Test workflow creation with detailed validation."""
        self.log("INFO", "ğŸ”§ Creating and validating fixed workflow definition")
        
        try:
            workflow = create_fixed_incident_workflow()
            workflow_dict = workflow.to_dict()
            self.log("SUCCESS", f"âœ… Fixed workflow created: {workflow_dict['name']}")
            
            # Validate workflow structure
            required_fields = ['name', 'description', 'steps', 'type']
            for field in required_fields:
                if field not in workflow_dict:
                    self.log("ERROR", f"âŒ Missing required field: {field}")
                    return None
                self.log("INFO", f"âœ“ {field}: {workflow_dict[field] if field != 'steps' else f'{len(workflow_dict[field])} steps'}")
            
            # Validate each step
            self.log("INFO", "ğŸ” Validating workflow steps:")
            for i, step in enumerate(workflow_dict['steps'], 1):
                step_name = step.get('name', f'step-{i}')
                self.log("STEP", f"  {i}. {step_name}")
                
                # Check step structure
                if 'command' not in step and 'executor' not in step:
                    self.log("WARNING", f"    âš ï¸ Step may be missing execution command")
                else:
                    self.log("INFO", f"    âœ“ Execution method configured")
                
                if 'depends' in step:
                    self.log("INFO", f"    ğŸ“ Depends on: {step['depends']}")
                
                if 'output' in step:
                    self.log("INFO", f"    ğŸ“¤ Output: {step['output']}")
            
            return workflow_dict
            
        except Exception as e:
            self.log("ERROR", f"âŒ Workflow creation failed: {str(e)}")
            return None
    
    def create_test_incident(self):
        """Create a comprehensive test incident with all fields."""
        incident_event = {
            "id": f"INC-2024-DEBUG-{int(time.time())}",
            "title": "DEBUG: Production Payment Gateway Critical Database Connection Crisis",
            "url": f"https://app.datadoghq.com/incidents/INC-2024-DEBUG-{int(time.time())}",
            "severity": "critical",
            "body": "ğŸš¨ CRITICAL PRODUCTION INCIDENT ğŸš¨\n\nPayment gateway experiencing catastrophic failure:\n- Error rate: 35% (threshold: 2%)\n- Response time: 4.2s (SLA: 500ms)\n- Database connections: 98% capacity\n- Failed transactions: 2,847\n- Revenue impact: $47,000\n- Customer complaints: 156\n\nTiming correlates with:\n- Payment service v2.4.1 deployment (52 minutes ago)\n- Traffic spike +65% from flash sale\n- Database maintenance window completed 1hr ago\n\nREQUIRES IMMEDIATE INVESTIGATION AND POTENTIAL ROLLBACK",
            "kubiya": {
                "slack_channel_id": f"#inc-debug-{int(time.time())}-payment-crisis"
            }
        }
        
        self.log("INFO", "ğŸ“‹ Created comprehensive test incident")
        self.log("INFO", f"  ğŸ†” ID: {incident_event['id']}")
        self.log("INFO", f"  ğŸ“ Title: {incident_event['title']}")
        self.log("INFO", f"  ğŸš¨ Severity: {incident_event['severity']}")
        self.log("INFO", f"  ğŸ’¬ Slack Channel: {incident_event['kubiya']['slack_channel_id']}")
        
        return incident_event
    
    def execute_with_streaming(self, workflow_dict: dict, incident_event: dict):
        """Execute workflow with comprehensive streaming and monitoring."""
        self.log("INFO", "ğŸš€ Starting workflow execution with streaming...")
        self.start_time = time.time()
        
        workflow_params = {
            "event": json.dumps(incident_event)
        }
        
        self.log("INFO", f"ğŸ“¦ Workflow parameters prepared")
        self.log("INFO", f"  ğŸ“„ Event size: {len(workflow_params['event'])} characters")
        
        try:
            # Execute with streaming enabled
            self.log("INFO", "ğŸŒŠ Enabling streaming execution...")
            
            for event in self.client.execute_workflow(
                workflow_definition=workflow_dict,
                parameters=workflow_params,
                stream=True
            ):
                self.events_received += 1
                self.process_streaming_event(event)
                
                # Limit events for debugging (remove in production)
                if self.events_received >= 20:
                    self.log("WARNING", "âš ï¸ Limiting events for debugging (first 20 shown)")
                    break
            
            self.log("SUCCESS", f"âœ… Streaming completed - Total events: {self.events_received}")
            
        except Exception as e:
            self.log("ERROR", f"âŒ Streaming execution failed: {str(e)}")
            import traceback
            self.log("ERROR", f"ğŸ” Traceback: {traceback.format_exc()}")
    
    def execute_without_streaming(self, workflow_dict: dict, incident_event: dict):
        """Execute workflow without streaming for comparison."""
        self.log("INFO", "ğŸ¯ Executing workflow without streaming...")
        
        workflow_params = {
            "event": json.dumps(incident_event)
        }
        
        try:
            result = self.client.execute_workflow(
                workflow_definition=workflow_dict,
                parameters=workflow_params,
                stream=False
            )
            
            self.log("SUCCESS", "âœ… Non-streaming execution completed")
            self.analyze_final_result(result)
            return result
            
        except Exception as e:
            self.log("ERROR", f"âŒ Non-streaming execution failed: {str(e)}")
            import traceback
            self.log("ERROR", f"ğŸ” Traceback: {traceback.format_exc()}")
            return None
    
    def process_streaming_event(self, event):
        """Process and analyze each streaming event."""
        self.log("EVENT", f"ğŸ“¡ Event #{self.events_received}")
        
        if isinstance(event, dict):
            # Extract event details
            event_type = event.get('type', 'unknown')
            step_name = event.get('step_name', event.get('stepName', 'unknown'))
            status = event.get('status', event.get('state', 'unknown'))
            message = event.get('message', event.get('msg', ''))
            
            self.log("EVENT", f"  ğŸ“‹ Type: {event_type}")
            self.log("EVENT", f"  ğŸ”§ Step: {step_name}")
            self.log("EVENT", f"  ğŸ“Š Status: {status}")
            
            if message:
                self.log("EVENT", f"  ğŸ’¬ Message: {message[:100]}...")
            
            # Track step status
            if step_name != 'unknown':
                self.step_statuses[step_name] = {
                    'status': status,
                    'type': event_type,
                    'timestamp': time.time(),
                    'event_number': self.events_received
                }
            
            # Look for specific event types
            if event_type in ['step.started', 'step.running']:
                self.log("STEP", f"â–¶ï¸ Step starting: {step_name}")
            elif event_type in ['step.completed', 'step.success']:
                self.log("STEP", f"âœ… Step completed: {step_name}")
            elif event_type in ['step.failed', 'step.error']:
                self.log("STEP", f"âŒ Step failed: {step_name}")
            elif event_type in ['workflow.completed', 'workflow.success']:
                self.log("SUCCESS", f"ğŸ‰ Workflow completed successfully!")
            elif event_type in ['workflow.failed', 'workflow.error']:
                self.log("ERROR", f"ğŸ’¥ Workflow failed!")
            
            # Show output data if available
            if 'output' in event and event['output']:
                output_preview = str(event['output'])[:150]
                self.log("EVENT", f"  ğŸ“¤ Output: {output_preview}...")
        
        elif isinstance(event, str):
            # Handle string events
            self.log("EVENT", f"  ğŸ“ String event: {event[:100]}...")
        
        else:
            # Handle other event types
            self.log("EVENT", f"  â“ Unknown event type: {type(event)}")
    
    def analyze_final_result(self, result):
        """Analyze the final workflow result in detail."""
        self.log("INFO", "ğŸ“Š Analyzing final workflow result...")
        
        if not result:
            self.log("WARNING", "âš ï¸ No final result received")
            return
        
        self.log("INFO", f"ğŸ“‹ Result type: {type(result)}")
        
        if isinstance(result, dict):
            # Check execution metadata
            for key in ['execution_id', 'id', 'executionId', 'status', 'state']:
                if key in result:
                    self.log("INFO", f"  ğŸ”‘ {key}: {result[key]}")
            
            # Check for errors
            if 'errors' in result and result['errors']:
                self.log("ERROR", f"âŒ Errors found: {len(result['errors'])}")
                for i, error in enumerate(result['errors'][:3], 1):
                    self.log("ERROR", f"  {i}. {error}")
            
            # Check for outputs
            if 'outputs' in result and result['outputs']:
                self.log("SUCCESS", f"ğŸ“¤ Outputs available: {len(result['outputs'])}")
                for output_name, output_value in result['outputs'].items():
                    output_preview = str(output_value)[:100]
                    self.log("INFO", f"  ğŸ“„ {output_name}: {output_preview}...")
            
            # Check for events
            if 'events' in result and result['events']:
                self.log("INFO", f"ğŸ“¡ Events in result: {len(result['events'])}")
                for i, event in enumerate(result['events'][:3], 1):
                    event_preview = str(event)[:80]
                    self.log("INFO", f"  {i}. {event_preview}...")
        
        # Show full result (truncated)
        result_str = json.dumps(result, indent=2, default=str)
        if len(result_str) > 1000:
            result_str = result_str[:1000] + "... [truncated]"
        
        self.log("INFO", f"ğŸ“‹ Full result preview:\n{result_str}")
    
    def generate_execution_report(self):
        """Generate a comprehensive execution report."""
        duration = time.time() - self.start_time if self.start_time else 0
        
        print("\n" + "="*80)
        print("ğŸ“Š COMPREHENSIVE EXECUTION REPORT")
        print("="*80)
        
        print(f"â±ï¸  **Execution Duration**: {duration:.2f} seconds")
        print(f"ğŸ“¡ **Events Received**: {self.events_received}")
        print(f"ğŸ”§ **Steps Tracked**: {len(self.step_statuses)}")
        
        if self.step_statuses:
            print(f"\nğŸ“‹ **Step Status Summary**:")
            for step_name, step_data in self.step_statuses.items():
                status_emoji = "âœ…" if "complet" in step_data['status'].lower() or "success" in step_data['status'].lower() else "âŒ" if "fail" in step_data['status'].lower() or "error" in step_data['status'].lower() else "â³"
                print(f"  {status_emoji} {step_name}: {step_data['status']} (Event #{step_data['event_number']})")
        
        print(f"\nğŸ“ˆ **Event Timeline**:")
        for i, log_entry in enumerate(self.execution_log[-10:], 1):  # Show last 10 log entries
            print(f"  {i}. [{log_entry['timestamp']}] {log_entry['level']}: {log_entry['message']}")
        
        print(f"\nğŸ¯ **Validation Results**:")
        expected_steps = [
            "parse-incident-event",
            "get-slack-token", 
            "get-secrets",
            "create-incident-channel",
            "claude-code-investigation",
            "update-slack-results"
        ]
        
        for step in expected_steps:
            if step in self.step_statuses:
                status = self.step_statuses[step]['status']
                emoji = "âœ…" if "success" in status.lower() or "complet" in status.lower() else "âŒ" if "fail" in status.lower() else "â³"
                print(f"  {emoji} {step}: {status}")
            else:
                print(f"  â“ {step}: No status received")


def main():
    """Enhanced main function with comprehensive debugging."""
    print("ğŸ” ENHANCED INCIDENT RESPONSE WORKFLOW DEBUGGER")
    print("ğŸ¯ Real-time streaming, detailed logging, step-by-step verification")
    print("="*90)
    
    # Check API key
    api_key = os.getenv('KUBIYA_API_KEY')
    if not api_key:
        print("âŒ KUBIYA_API_KEY environment variable not set")
        return 1
    
    print(f"âœ… API Key available (length: {len(api_key)})")
    
    # Initialize debugger
    debugger = WorkflowDebugger(api_key)
    debugger.log("INFO", "ğŸš€ Enhanced workflow debugger initialized")
    
    # Test workflow creation
    workflow_dict = debugger.test_workflow_creation()
    if not workflow_dict:
        debugger.log("ERROR", "âŒ Cannot proceed without valid workflow")
        return 1
    
    # Create test incident
    incident_event = debugger.create_test_incident()
    
    # Execute with streaming
    debugger.log("INFO", "ğŸŒŠ Testing with streaming enabled...")
    debugger.execute_with_streaming(workflow_dict, incident_event)
    
    # Execute without streaming for comparison
    debugger.log("INFO", "ğŸ¯ Testing without streaming for comparison...")
    final_result = debugger.execute_without_streaming(workflow_dict, incident_event)
    
    # Generate comprehensive report
    debugger.generate_execution_report()
    
    print("\nğŸ‰ ENHANCED DEBUGGING COMPLETE!")
    print("="*90)
    print("âœ… **Streaming Events**: Real-time monitoring enabled")
    print("âœ… **Step Tracking**: Individual step status monitored")
    print("âœ… **Error Detection**: Comprehensive error analysis")
    print("âœ… **Performance Metrics**: Timing and throughput measured")
    print("âœ… **Validation**: All expected steps verified")
    
    return 0


if __name__ == "__main__":
    sys.exit(main())