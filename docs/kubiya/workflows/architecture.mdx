---
title: "Workflow Architecture"
description: "Deep dive into Kubiya's serverless, containerized workflow execution model"
icon: "server"
---

# Serverless Workflow Architecture

Kubiya implements a cutting-edge serverless architecture where every workflow and step runs as an independent Docker container. This design enables unprecedented flexibility, scalability, and software compatibility.

## Core Principles

### 1. **Containerized Everything**

<Card title="ðŸ³ Docker-Based Execution" icon="docker">
  Every single step in your workflow runs in its own Docker container:
  
  - **Any Language**: Python, JavaScript, Go, Rust, Java, C++, Ruby, etc.
  - **Any Tool**: Git, AWS CLI, Terraform, kubectl, npm, cargo, etc.
  - **Any Library**: Install and use any package or dependency
  - **Any Version**: Pin specific versions of languages and tools
</Card>

### 2. **True Statelessness**

Each workflow execution is completely independent:

```python
# This workflow runs in a fresh environment every time
workflow = Workflow(
    name="data-pipeline",
    runner="kubiya-hosted"
)

# Step 1 runs in container A
workflow.add_step(
    name="fetch-data",
    image="python:3.11",
    code="# This container is destroyed after execution"
)

# Step 2 runs in container B (completely separate)
workflow.add_step(
    name="process-data",
    image="python:3.11",
    code="# Fresh container, no state from Step 1"
)
```

### 3. **Infinite Scalability**

The serverless model means:
- No pre-provisioned resources
- Automatic scaling based on demand
- Pay only for actual execution time
- Handle 1 or 1,000,000 workflows seamlessly

## Architecture Diagram

```mermaid
graph TB
    subgraph "Workflow Definition"
        W[Workflow YAML/Code]
    end
    
    subgraph "ADK Orchestration Layer"
        O[ADK Orchestrator]
        Q[Execution Queue]
        S[State Manager]
    end
    
    subgraph "Serverless Execution Layer"
        subgraph "Step 1"
            C1[Docker Container 1]
            R1[Resources]
        end
        
        subgraph "Step 2"
            C2[Docker Container 2]
            R2[Resources]
        end
        
        subgraph "Step N"
            CN[Docker Container N]
            RN[Resources]
        end
    end
    
    subgraph "Storage Layer"
        A[Artifact Storage]
        L[Logs]
        M[Metrics]
    end
    
    W --> O
    O --> Q
    Q --> C1
    Q --> C2
    Q --> CN
    
    C1 --> A
    C2 --> A
    CN --> A
    
    C1 --> L
    C2 --> L
    CN --> L
    
    O --> S
    S --> M
    
    style O fill:#f9f,stroke:#333,stroke-width:4px
    style C1 fill:#bbf,stroke:#333,stroke-width:2px
    style C2 fill:#bbf,stroke:#333,stroke-width:2px
    style CN fill:#bbf,stroke:#333,stroke-width:2px
```

## How It Works

### 1. **Workflow Definition**
You define workflows using Python SDK, YAML, or through AI generation:

```python
from kubiya_workflow_sdk import Workflow, Step

workflow = Workflow(
    name="multi-tool-pipeline",
    runner="kubiya-hosted"
)
```

### 2. **Container Specification**
Each step specifies its container requirements:

```python
# Use pre-built images
workflow.add_step(Step(
    name="python-analysis",
    image="python:3.11-slim",
    packages=["pandas", "numpy", "scikit-learn"],
    code="..."
))

# Or custom images
workflow.add_step(Step(
    name="custom-tool",
    image="myregistry/my-tool:latest",
    code="..."
))
```

### 3. **Execution**
When a workflow runs:

1. ADK orchestrator receives the workflow
2. Each step is queued for execution
3. Containers are spun up on-demand
4. Code executes in isolated environment
5. Results are captured and stored
6. Container is destroyed

### 4. **Data Flow**
Steps communicate through artifacts:

```python
# Step 1: Generate data
workflow.add_step(Step(
    name="generate",
    code="""
    data = create_dataset()
    save_artifact('dataset.csv', data)
    """
))

# Step 2: Process data
workflow.add_step(Step(
    name="process",
    code="""
    data = load_artifact('dataset.csv')
    results = analyze(data)
    save_artifact('results.json', results)
    """
))
```

## Real-World Examples

### Multi-Language ETL Pipeline

```python
workflow = Workflow(
    name="advanced-etl",
    runner="kubiya-hosted"
)

# Extract with Python
workflow.add_step(Step(
    name="extract",
    image="python:3.11",
    packages=["requests", "boto3"],
    code="""
    import requests
    import boto3
    
    # Fetch from API
    data = requests.get('https://api.example.com/data').json()
    
    # Also pull from S3
    s3 = boto3.client('s3')
    s3_data = s3.get_object(Bucket='my-bucket', Key='data.json')
    
    # Combine and save
    combined = merge_data(data, s3_data)
    save_artifact('raw_data.json', combined)
    """
))

# Transform with Node.js
workflow.add_step(Step(
    name="transform",
    image="node:20",
    packages=["lodash", "moment"],
    code="""
    const _ = require('lodash');
    const moment = require('moment');
    
    const data = loadArtifact('raw_data.json');
    
    // Complex transformations
    const transformed = _.chain(data)
        .filter(item => moment(item.date).isAfter('2024-01-01'))
        .groupBy('category')
        .mapValues(items => calculateMetrics(items))
        .value();
    
    saveArtifact('transformed.json', transformed);
    """
))

# Load with Go for performance
workflow.add_step(Step(
    name="load",
    image="golang:1.21",
    code="""
    package main
    
    import (
        "encoding/json"
        "database/sql"
        _ "github.com/lib/pq"
    )
    
    func main() {
        // Load transformed data
        data := loadArtifact("transformed.json")
        
        // Bulk insert with Go's concurrency
        db, _ := sql.Open("postgres", getConnString())
        
        // Use goroutines for parallel inserts
        chunks := splitIntoChunks(data, 1000)
        var wg sync.WaitGroup
        
        for _, chunk := range chunks {
            wg.Add(1)
            go func(c []Record) {
                defer wg.Done()
                bulkInsert(db, c)
            }(chunk)
        }
        
        wg.Wait()
    }
    """
))
```

### DevOps Automation

```python
workflow = Workflow(
    name="deploy-infrastructure",
    runner="kubiya-hosted"
)

# Terraform for infrastructure
workflow.add_step(Step(
    name="provision",
    image="hashicorp/terraform:latest",
    code="""
    terraform init
    terraform plan -out=tfplan
    terraform apply -auto-approve tfplan
    
    # Export outputs
    terraform output -json > outputs.json
    save_artifact('infra_outputs.json', 'outputs.json')
    """
))

# Ansible for configuration
workflow.add_step(Step(
    name="configure",
    image="ansible/ansible:latest",
    code="""
    # Load infrastructure details
    HOSTS=$(cat /artifacts/infra_outputs.json | jq -r '.hosts.value')
    
    # Run playbook
    ansible-playbook -i "$HOSTS" configure.yml
    """
))

# Kubernetes deployment
workflow.add_step(Step(
    name="deploy",
    image="bitnami/kubectl:latest",
    code="""
    # Apply manifests
    kubectl apply -f k8s/
    
    # Wait for rollout
    kubectl rollout status deployment/my-app
    
    # Run smoke tests
    kubectl run test --image=curlimages/curl -- http://my-app/health
    """
))
```

## Advanced Features

### Custom Base Images

Create specialized images for your workflows:

```dockerfile
# Dockerfile
FROM python:3.11
RUN pip install pandas numpy scikit-learn tensorflow
RUN apt-get update && apt-get install -y graphviz
COPY models/ /opt/models/
```

```python
workflow.add_step(Step(
    name="ml-pipeline",
    image="myregistry/ml-base:latest",
    code="# Use pre-installed ML tools"
))
```

### Resource Management

Control container resources:

```python
workflow.add_step(Step(
    name="memory-intensive",
    image="python:3.11",
    resources={
        "memory": "8Gi",
        "cpu": "4",
        "gpu": "1"  # For ML workloads
    },
    code="# Run memory-intensive operations"
))
```

### Network Isolation

Each container runs in isolation:

```python
# Step 1: Start a service
workflow.add_step(Step(
    name="api-server",
    image="node:20",
    ports=[3000],
    code="# Start API server"
))

# Step 2: Different network namespace
workflow.add_step(Step(
    name="client",
    image="python:3.11",
    code="# Cannot access Step 1's ports directly"
))
```

## AI-Powered Generation

<Note>
  With ADK orchestration, you can generate these complex workflows using natural language:
</Note>

```python
from kubiya_workflow_sdk import KubiyaWorkflow

# Generate entire workflow from description
workflow = KubiyaWorkflow.from_prompt(
    """
    Create a data pipeline that:
    1. Extracts data from PostgreSQL and MongoDB
    2. Transforms using Python pandas
    3. Runs ML predictions with TensorFlow
    4. Stores results in S3 and sends Slack notification
    """,
    runner="kubiya-hosted"
)

# ADK generates the complete containerized workflow
result = workflow.execute()
```

## Benefits Summary

<CardGroup cols={2}>
  <Card title="ðŸš€ Any Software" icon="cube">
    Run literally any software, tool, or language in your workflows
  </Card>
  
  <Card title="ðŸ“¦ Zero Dependencies" icon="box">
    No need to pre-install anything - containers have everything
  </Card>
  
  <Card title="ðŸ”„ Perfect Isolation" icon="shield">
    Each step runs in complete isolation with no side effects
  </Card>
  
  <Card title="âš¡ Instant Scale" icon="bolt">
    From 1 to 1 million executions without infrastructure changes
  </Card>
</CardGroup>

## Security & Compliance

The containerized architecture provides:

- **Process Isolation**: Each step runs in its own namespace
- **Resource Limits**: Prevent runaway processes
- **Network Policies**: Control communication between steps
- **Audit Trails**: Complete execution history
- **Secrets Management**: Secure credential injection

## What's Next?

<Card title="Getting Started" href="/getting-started/quickstart" icon="rocket">
  Create your first containerized workflow
</Card>

<Card title="ADK Provider" href="/providers/adk/getting-started" icon="robot">
  Learn about AI-powered workflow generation
</Card>

<Card title="Examples" href="/workflows/examples" icon="code">
  See real-world containerized workflows
</Card> 